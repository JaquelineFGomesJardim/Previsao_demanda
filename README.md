# ğŸ“¦ Projeto de PrevisÃ£o de Demanda com Data Lake em Camadas

Este projeto demonstra a construÃ§Ã£o de um pipeline de dados baseado em um **Data Lake estruturado por camadas**, com foco na **previsÃ£o de demanda de produtos**. Foi desenvolvido com boas prÃ¡ticas de engenharia e ciÃªncia de dados, simulando um ambiente produtivo.

---

## ğŸ¯ Objetivo

Criar uma arquitetura robusta de ingestÃ£o, transformaÃ§Ã£o e disponibilizaÃ§Ã£o de dados para suportar anÃ¡lises e modelagem preditiva, com foco em:

- GovernanÃ§a e rastreabilidade dos dados
- Modularidade e reuso em diferentes estÃ¡gios do pipeline
- PreparaÃ§Ã£o eficiente de dados para modelos de machine learning

---

## ğŸ—ï¸ Arquitetura do Data Lake

O Data Lake Ã© organizado em **camadas** com responsabilidades bem definidas:

| Camada     | DescriÃ§Ã£o |
|------------|-----------|
| **Ingestion** | Armazena os **dados brutos originais**, exatamente como foram recebidos. |
| **Raw** | Dados com **pequenas padronizaÃ§Ãµes**, como renomeaÃ§Ã£o de colunas e conversÃ£o de tipos. |
| **Trusted** | Camada onde ocorre a **engenharia de variÃ¡veis** com regras de negÃ³cio e agregaÃ§Ãµes temporais. |
| **Refined** | Dados **prontos para consumo analÃ­tico ou alimentar modelos** de machine learning. |
| **Metadados** | DocumentaÃ§Ã£o automatizada com estatÃ­sticas descritivas de cada variÃ¡vel (nulos, tipos, cardinalidade, etc). |

---

## âš™ï¸ Funcionalidades Implementadas

- ğŸ“¥ IngestÃ£o simulada a partir de arquivos `.csv` e `.parquet`
- ğŸ§¼ PadronizaÃ§Ã£o de dados e tratamento de tipos
- ğŸ§  Feature engineering com janelas de tempo, agregaÃ§Ãµes e frequÃªncias
- ğŸ“Š AnÃ¡lises exploratÃ³rias para apoiar a modelagem
- ğŸ§¾ GeraÃ§Ã£o de metadados automatizados por DataFrame
- ğŸ’¾ Escrita de dados em formato Parquet por camadas organizadas
- ğŸ§ª ConstruÃ§Ã£o de janelas de observaÃ§Ã£o e previsÃ£o para modelagem futura

---

## ğŸ§° Tecnologias e Bibliotecas Utilizadas

- **Apache Spark (PySpark)** â€“ Processamento de dados em larga escala
- **Python 3.10+** â€“ Linguagem base do projeto
- **Java JDK 11** â€“ Requisito para o funcionamento do Spark
- **Google Colab** â€“ Ambiente de desenvolvimento e execuÃ§Ã£o
- **Google Drive** â€“ SimulaÃ§Ã£o de armazenamento em nuvem
- **findspark** â€“ IntegraÃ§Ã£o entre Spark e Colab
- **pandas / matplotlib / seaborn** â€“ AnÃ¡lise e visualizaÃ§Ã£o de dados

---

## ğŸ—‚ï¸ Estrutura de DiretÃ³rios

````bash
DADOS/
â”œâ”€â”€ INGESTION/       # Dados brutos originais
â”œâ”€â”€ RAW/             # Dados padronizados
â”œâ”€â”€ TRUSTED/         # Feature engineering e regras de negÃ³cio
â”œâ”€â”€ REFINED/         # Dados prontos para consumo analÃ­tico/modelagem
â””â”€â”€ METADADOS/       # EstatÃ­sticas e dicionÃ¡rios de variÃ¡veis
````

# ğŸ§ª ExecuÃ§Ã£o no Google Colab

Para configurar o ambiente,basta executar essa linha:
!pip install -q findspark pyspark

Caso o processamento seja feito em outro local, precisa instalar as versÃµes utilizadas, abaixo:

# Instalar Java e Spark
!apt-get install openjdk-11-jdk-headless -qq

!wget -q https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz

!tar xf spark-3.3.2-bin-hadoop3.tgz



