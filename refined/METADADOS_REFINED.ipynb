{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPQtNu/gdAEtoTn+f1awNBM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YYix7svkFx1i","executionInfo":{"status":"ok","timestamp":1750352215036,"user_tz":-60,"elapsed":942,"user":{"displayName":"Jaqueline Jardim","userId":"03616687769247133499"}},"outputId":"db903666-37c0-4d5b-849e-780f8e9ad98e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["!pip install -q findspark pyspark"],"metadata":{"id":"sQRsony7F03z","executionInfo":{"status":"ok","timestamp":1750352224449,"user_tz":-60,"elapsed":8536,"user":{"displayName":"Jaqueline Jardim","userId":"03616687769247133499"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Inicializa Spark\n","\n","import findspark\n","findspark.init()\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder \\\n","    .appName(\"GenerateRefinedMetadata\") \\\n","    .getOrCreate()"],"metadata":{"id":"SFzL1okWHpxB","executionInfo":{"status":"ok","timestamp":1750353161651,"user_tz":-60,"elapsed":221,"user":{"displayName":"Jaqueline Jardim","userId":"03616687769247133499"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F"],"metadata":{"id":"7C6ia5KzHVMV","executionInfo":{"status":"ok","timestamp":1750353162283,"user_tz":-60,"elapsed":2,"user":{"displayName":"Jaqueline Jardim","userId":"03616687769247133499"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def generate_metadados(dataframe, df_name: str):\n","    \"\"\"\n","    Gera metadados de `dataframe` e adiciona em todas as linhas\n","    uma coluna 'Warehouse' com o nome do DataFrame de origem.\n","    \"\"\"\n","    # 1) coleta as estatísticas originais\n","    column_names  = dataframe.columns\n","    dtypes        = [dtype for _, dtype in dataframe.dtypes]\n","    total_count   = dataframe.count()\n","    null_counts   = [dataframe.filter(F.col(c).isNull()).count() for c in column_names]\n","    null_percents = [round((cnt/total_count)*100, 2) for cnt in null_counts]\n","    cardinality   = [dataframe.select(c).distinct().count() for c in column_names]\n","\n","    # monta o DataFrame de metadados\n","    metadata = spark.createDataFrame(\n","        zip(column_names, dtypes, null_counts, null_percents, cardinality),\n","        schema=[\"nome_variavel\", \"tipo\", \"qt_nulos\", \"percent_nulos\", \"cardinalidade\"]\n","    ).orderBy(F.desc(\"percent_nulos\"))\n","\n","    # Coluna de identificação\n","    metadata = metadata.withColumn(\"Warehouse\", F.lit(df_name))\n","\n","    # Warehouse é para que o nome do df seja a primeira coluna\n","    cols = [\"Warehouse\"] + [c for c in metadata.columns if c != \"Warehouse\"]\n","    return metadata.select(*cols)"],"metadata":{"id":"-qWuauYsHJ1i","executionInfo":{"status":"ok","timestamp":1750353163226,"user_tz":-60,"elapsed":7,"user":{"displayName":"Jaqueline Jardim","userId":"03616687769247133499"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jO4IZSG9Fiqn","executionInfo":{"status":"ok","timestamp":1750353574050,"user_tz":-60,"elapsed":319952,"user":{"displayName":"Jaqueline Jardim","userId":"03616687769247133499"}},"outputId":"69bdaada-f2b0-411a-c2be-4b395843ef93"},"outputs":[{"output_type":"stream","name":"stdout","text":["✔ Metadados METADADOS_DF_J_train salvos em /content/drive/MyDrive/PREVISAO_DEMANDA/REFINED/METADADOS_REFINED/METADADOS_DF_J_train\n","✔ Metadados METADADOS_DF_J_val salvos em /content/drive/MyDrive/PREVISAO_DEMANDA/REFINED/METADADOS_REFINED/METADADOS_DF_J_val\n","✔ Metadados METADADOS_DF_A_train salvos em /content/drive/MyDrive/PREVISAO_DEMANDA/REFINED/METADADOS_REFINED/METADADOS_DF_A_train\n","✔ Metadados METADADOS_DF_A_val salvos em /content/drive/MyDrive/PREVISAO_DEMANDA/REFINED/METADADOS_REFINED/METADADOS_DF_A_val\n","✔ Metadados METADADOS_DF_S_train salvos em /content/drive/MyDrive/PREVISAO_DEMANDA/REFINED/METADADOS_REFINED/METADADOS_DF_S_train\n","✔ Metadados METADADOS_DF_S_val salvos em /content/drive/MyDrive/PREVISAO_DEMANDA/REFINED/METADADOS_REFINED/METADADOS_DF_S_val\n","✔ Metadados METADADOS_DF_C_train salvos em /content/drive/MyDrive/PREVISAO_DEMANDA/REFINED/METADADOS_REFINED/METADADOS_DF_C_train\n","✔ Metadados METADADOS_DF_C_val salvos em /content/drive/MyDrive/PREVISAO_DEMANDA/REFINED/METADADOS_REFINED/METADADOS_DF_C_val\n"]}],"source":["refined_base = \"/content/drive/MyDrive/PREVISAO_DEMANDA/REFINED\"\n","out_base     = os.path.join(refined_base, \"METADADOS_REFINED\")\n","\n","warehouses = ['J','A','S','C']\n","splits     = ['train','val']\n","\n","for wh in warehouses:\n","    for split in splits:\n","        # 1) Lê o DataFrame refinado\n","        path = os.path.join(refined_base, f\"FEATURES_{wh}\", split)\n","        df   = spark.read.parquet(path)\n","\n","        # 2) (Opcional) cache para acelerar\n","        df.cache().count()\n","\n","        # 3) Gera metadados\n","        source_name = f\"METADADOS_DF_{wh}_{split}\"\n","        md = generate_metadados(df, source_name)\n","\n","        # 4) Define saída em METADADOS_REFINED/METADADOS_DF_{wh}_{split}\n","        out_path = os.path.join(out_base, source_name)\n","\n","        # 5) Grava em CSV com header (um arquivo)\n","        md.coalesce(1) \\\n","          .write \\\n","          .mode(\"overwrite\") \\\n","          .option(\"header\", True) \\\n","          .csv(out_path)\n","\n","        print(f\"✔ Metadados {source_name} salvos em {out_path}\")\n","\n","spark.stop()"]}]}